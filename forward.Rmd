# Forward Mode

The forward-mode automatic differentiation algorithm, also known as
the tangent method, is a means for efficiently computing derivatives
of smooth functions $f:\mathbb{R} \rightarrow \mathbb{R}^M$
with a single input and multiple outputs.

Suppose $x \in \mathbb{R}$ and that $v = f(u)$.  We write a
dot over an expression to indicate a derivative with respect to a
variable $x$,
$$
\dot{u} = \frac{\partial}{\partial x} u.
$$
The term $\dot{u}$ is called the tangent of $u$ with respect to $x$;
the $x$ is implicit in the notation, but assumed to be the same $x$ in
expressions with multiple dotted expressions.

For example, if $v = -u$, then by the chain rule,
$$
\dot{v}
= \frac{\partial}{\partial x} v
= \frac{\partial}{\partial x} -u
= - \frac{\partial}{\partial x} u
= -\dot{u}.
$$
Similarly, if we have $v = \exp(u)$, then
$$
\dot{v}
= \frac{\partial}{\partial x} v
= \frac{\partial}{\partial x} \exp(u)
= \exp(u) \cdot \frac{\partial}{\partial x} u
= \exp(u) \cdot \dot{u}.
$$

Derivative propagatin in forward mode works the same way for
multivariate functions.  For example, if $y = u \cdot v$ is a product,
the usual derivative rule for products applies,
$$
\dot{y}
= \frac{\partial}{\partial x} u \cdot v
= \left( \frac{\partial}{\partial x} u \right) \cdot v
+ u \cdot \left( \frac{\partial}{\partial x} v \right)
= \dot{u} \cdot v + u \cdot \dot{v}.
$$

## Dual numbers

Forward-mode automatic differentiation may be formalized using dual
numbers consisting of the value of an expression and its derivative,
$\langle u, \dot{u} \rangle.$  Smooth functions may then be extended
to operate on dual numbers.  For example, negation is defined for dual
numbers by
$$
-\langle u, \dot{u}\rangle = \langle -u, -\dot{u} \rangle.
$$
For sums,
$$
\langle u, \dot{u} \rangle + \langle v, \dot{v} \rangle
= \langle u + v, \dot{u} + \dot{v} \rangle,
$$
and for differences
$$
\langle u, \dot{u} \rangle - \langle v, \dot{v} \rangle
= \langle u - v, \dot{u} - \dot{v} \rangle,
$$
For products,
$$
\langle u, \dot{u} \rangle \cdot \langle v, \dot{v} \rangle
= \langle u \cdot v, \dot{u} \cdot v + u \cdot \dot{v} \rangle
$$
and for quotients,
$$
\langle u, \dot{u} \rangle / \langle v, \dot{v} \rangle
= \langle u / v, \dot{u} / v - u / v^2 \cdot \dot{v} \rangle.
$$
For exponentiation,
$$
\exp\left( \langle u, \dot{u} \rangle \right)
= \langle \exp(u), \exp(u) \cdot \dot{u}\rangle,
$$
and for the logarithm,
$$
\log \langle u, \dot{u} \rangle
=
\langle \log u, \frac{1}{u} \cdot \dot{u} \rangle.
$$
These all translate directly into rules of tangent propagation.

## Vectors and matrices

The definitions of values and tangents remain the same when moving to
vector or matrix functions.  If $A$ is an $M \times N$ matrix, then
$\dot{A}$ is the $M \times N$ matrix composed of the tangents of the
variables in $A$, i.e., $\dot{A}[i, j]$ is the tangent of matrix entry
$A[i, j]$.

The tangent rules for matrix operations carry over neatly from the
scalar case.  For example, $C = A + B$ is the sum of two matrices, the
corresponding tangent rule is
$$
\dot{C} = \dot{A} + \dot{B}.
$$
Here and throughout, matrices used in arithmetic operations will be
assumed to conform to the required shape and size constraints in the
expressions in which they are used.  For $A + B$ to be well formed,
$A$ and $B$ must both be $M \times N$ matrices (i.e., they must have
the same number of rows and columns).

Similarly, if $C = A \cdot B$ is the product of two matrices, the
tangent rule is the same as that for scalars,
$$
\dot{C} = \dot{A} \cdot B + A \cdot \dot{B}.
$$

Simple tangent rules exist for many linear algebra operations, such as
inverse.  If $C = A^{-1}$, then the tangent rule is
$$
\dot{C} = -C \cdot \dot{A} \cdot C.
$$
Results such as these are derived through algebraic manipulation and
differentiation (see \cite{giles:2008b} for general rules).  For
inverse, because
$$
C \cdot A = A^{-1} \cdot A = \textrm{I}.
$$
Differentiating both sides yields
$$
\frac{\partial}{\partial x} C \cdot A
=
\frac{\partial}{\partial x} \textrm{I}.
$$
Replacing with dot notation yields
$$
\dot{C} \cdot A + C \cdot \dot{A} = 0.
$$
Rearranging the terms produces
$$
\dot{C} \cdot A = - C \cdot \dot{A}.
$$
Multiplying both sides of the equation on the right by $A^{-1}$ gives
$$
\dot{C} \cdot A \cdot A^{-1} = -C \cdot \dot{A} \cdot A^{-1}.
$$
This reduces to the final simplified form
$$
\dot{C} = -C \cdot \dot{A} \cdot C,
$$
after dropping the factor $A \cdot A^{-1} = \textrm{I}$ and replacing
$A^{-1}$ with its value $C$.


## Gradient-vector products

The product of the gradient of a function at a given point and an
arbitrary vector can be computed efficiently using forward-mode
automatic differentiation.  Suppose $f : \mathbb{R}^N \rightarrow
\mathbb{R}$ is a smooth function, $\nabla f(x)$ is its gradient at $x
\in \mathbb{R}^N$.  For a vector $v \in \mathbb{R}^N,$
the gradient-vector product is
$$
\nabla f(x) \cdot v
= \sum_{n = 1}^N
\nabla f_n(x) \cdot v_n
= \sum_{n = 1}^N \frac{\partial f(x)}{\partial x_n} \cdot v_n.
$$
The types work out because gradients are taken to be row vectors,
whereas $v$ is an ordinary column vector.

To compute gradient-vector products with forward-mode automatic
differentiation, the tangent of the input variable
$x$ needs to be initilialized with the vector being multiplied,
$$
\dot{x} = v.
$$
Then dual arithmetic is used as usual to compute the result, and the
final dual number's tangent is the gradient-vector product.

For example, suppose $f(x) = x_1 \cdot x_2 + x_2,$
$x = \begin{bmatrix} 12.9 & 127.1 \end{bmatrix}^{\top},$ and
$v = \begin{bmatrix} 0.3 & -1.2 \end{bmatrix}^{\top}.$  The
gradient-vector product is derived as
$$
\begin{array}{rcl}
\langle 12.9, 0.3 \rangle \cdot \langle 127.1, -1.2 \rangle
+ \langle 127.1, -1.2 \rangle
& = &
\langle 12.9 \cdot 127.1, \, 0.3 \cdot 127.1 + 12.9 \cdot -1.2 \rangle
+ \langle 127.1, -1.2 \rangle
\\[4pt]
& = &
\langle 12.9 \cdot 127.1 + 127.1, \
        0.3 \cdot 127.1 + 12.9 \cdot -1.2 + -1.2
\rangle
\\[4pt]
& = &
\langle 1767, 21 \rangle.
\end{array}
$$
Checking the gradient-vector product analytically,
$$
\nabla f(x)
=
\begin{bmatrix} x_2 & x_1 + 1 \end{bmatrix}
=
\begin{bmatrix} 127.1 & 12.9 + 1 \end{bmatrix},
$$
so that
$$
\nabla f(x) \cdot v
= \begin{bmatrix} 127.1 & 12.9 + 1 \end{bmatrix}
  \cdot  \begin{bmatrix} 0.3 & -1.2 \end{bmatrix}^{\top}
  = 21.
$$
