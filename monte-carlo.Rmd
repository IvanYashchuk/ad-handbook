# Monte Carlo Methods

Monte Carlo methods are used to compute high-dimensional integrals
corresponding to expectations.  For example, if $Y \in \mathbb{R}$ is
a real-valued random variable and $f:\mathbb{R} \rightarrow
\mathbb{R}$ is a real-valued function, then the expectation of the
function applied to the random variable is
$$
\mathbb{E}[f(y)]
= \int_{\mathbb{R}} f(y) \cdot p_Y(y) \, \textrm{d} y,
$$
where $p_Y(y)$ is the density function for $Y$.

## Introduction to Monte Carlo Methods

Suppose it possible to generate a sequence of random draws
$$
y^{(1)}, \ldots, y^{(m)}, \ldots \sim p_Y(y)
$$
distributed according to $p_Y(y)$.  With these random draws, the
expectation may be reformulated as as the limit of the average
value of the function applied to the draws,
$$
\mathbb{E}[f(y)]
=
\lim_{M \rightarrow \infty} \frac{1}{M} \sum_{m=1}^M f(y^{(m)}).
$$
Given finite computation time, an approximate result can be computed
for some fixed $M$ as
$$
\mathbb{E}[f(y)]
\approx \frac{1}{M} \sum_{m=1}^M f(y^{(m)}).
$$
This is known as a Monte Carlo method, after the casino of that name
in Monaco.  If the samples are independent or drawn from a geometrically
ergodic Markov chain, the approximation will converge according to the
central limit theorem at a rate of $\mathcal{O}(\sqrt{\frac{1}{M}}).$

## Sensitivities of expectations

Suppose that $p(y \mid \theta)$ is the distribution of a random
variable $Y \in \mathbb{R}$ and there is some random quantity of
interest $f(Y)$ whose expectation for some fixed parameter values
$\theta$,
$$
\mathbb{E}[f(Y) \mid \theta]
= \int f(y) \cdot p_{Y \mid \Theta}(y \mid \theta) \, \textrm{d}y,
$$
is of interest.

The derivative of this expectation with respect to the parameters
$\theta$,
$$
\nabla \mathbb{E}\left[f(Y) \mid \theta\right],
$$
quantifies the rate of change in the expectaiton of $f(Y)$ as $\theta$
changes.

## The Score function

The *score function* for the variable $Y$, which depends on parameters
$\Theta$, is
$$
\textrm{score}(\theta) = \nabla \log p_{Y \mid \Theta}(y \mid \theta).
$$
By the chain rule,
$$
\nabla \log p_{Y \mid \Theta}(y \mid \theta)
= \frac{1}{\displaystyle p_{Y \mid \Theta}(y \mid \theta)}
  \cdot \nabla p_{Y \mid \Theta}(y \mid \theta).
$$
Rearranging terms expresses the gradient of the density in terms of
the score function and density,
$$
\nabla p_{Y \mid \Theta}(y \mid \theta)
= p(y \mid \theta) \cdot \nabla \log p(x \mid \theta).
$$


## Monte Carlo gradients

The following derivation provides the basis for using Monte Carlo
methods to calculate gradients.
\begin{eqnarray*}
\nabla_{\theta} \, \mathbb{E}[f(Y) \mid \theta]
& = &
{\large \nabla}_{\theta} \int f(y)
            \cdot p_{Y \mid \Theta}(y \mid \theta)
       \, \textrm{d} y
\\[4pt]
& = &
\int f(y)
     \cdot \left( {\large \nabla}_{\theta} p_{Y \mid \Theta}(y \mid \theta) \right)
\, \textrm{d} y
\\[4pt]
& = &
\int f(y)
     \cdot \left( {\large \nabla}_{\theta} \, \log p_{Y \mid \Theta}(y \mid \theta) \right)
     \cdot p_{Y \mid \Theta}(y \mid \theta)
     \, \textrm{d} y
\\[4pt]
& = &
\mathbb{E}\left[f(Y) \cdot {\large \nabla}_{\theta} \log p(Y \mid \theta) \mid \theta \right]
\\[4pt]
& = &
\lim_{M \rightarrow \infty}
\frac{1}{M}
\cdot \sum_{m = 1}^M  y^{(m)}
                    \cdot {\large \nabla}_{\theta} \log p_{Y \mid \Theta}(y
		    \mid \theta)
\\[4pt]
& \approx &
\frac{1}{M}
\cdot \sum_{m = 1}^M y^{(m)}
                    \cdot {\large \nabla}_{\theta} \log p_{Y \mid \Theta}(y
		    \mid \theta),
\end{eqnarray*}
where $y^{(m)} \sim p_{Y \mid \Theta}(y \mid \theta).$ Because $Y$ is
random and $\mathbb{E}\left[f(Y) \mid \theta\right]$ is a conditional
expectation, the gradient is taken with respect to $\theta.$ The first
line follows from the the definition of expectations.  The second step
uses the dominated convergence theorem to distribute the gradient into
the integral (past the constant $f(y)$).  The third step uses the
score function derivation of the gradient of the density.  The fourth
step is again the definiton of an expectation, where again $\theta$ is
fixed.  The penultimate step involves the fundamental rule of Monte
Carlo integration, assuming $y^{(m)} \sim p_{Y \mid \Theta}(y \mid
\theta).$ The final step is the approximation resulting from using
only finitely many draws.
