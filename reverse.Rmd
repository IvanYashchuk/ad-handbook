# Reverse Mode

The reverse-mode automatic differentiation algorithm, also known as
the adjoint method, is a means for efficiently computing derivatives
of smooth functions $f : \mathbb{R}^N \rightarrow \mathbb{R}$ from
multiple inputs to a single output, i.e., computing gradients.

## Adjoints

Suppose $y \in \mathbb{R}$ is a final dependent variable and $v$ is a
subexpression used in the calculation of $y$.  The adjoint of $v$ is
the derivative of $y$ with respect to $v$, and is written with a bar,
$$
\bar{v} = \frac{\partial}{\partial v} y.
$$

Suppose that $v = f(u)$.  The chain rule tells us that
$$
\bar{u}
= \frac{\partial y}{\partial u}
= \frac{\partial y}{\partial v}
\cdot \frac{\partial v}{\partial u}
= \frac{\partial y}{\partial v}
\cdot \frac{\partial f(u)}{\partial u}
= \frac{\partial y}{\partial v}
\cdot f'(u) \cdot \frac{\partial u}{\partial u}
= \bar{v} \cdot f'(u).
$$
Because a sub-expression may be involved in more than one expression,
the adjoint rules are expressed as increments,
$$
\bar{u} \ \ {\small +}{=} \ \ \bar{v} \cdot f'(u).
$$

For example, if $v = \exp(u)$, with $\exp'(u) = \exp(u)$, the rule is
$$
\bar{u} \ \ {\small +}{=} \ \ \bar{v} \cdot \exp(u).
$$
For logarithms, with $v = \log u$, with $\log'(u) = \frac{1}{u},$ the
adjoint rule is
$$
\bar{u} \ \ {\small +}{=} \ \ \bar{v} \cdot \frac{1}{u}.
$$

When there is more than one argument, adjoints propagate independently
from the result by multiplying the result's adjoint times the partial
with respect to the argument.  For example, if $w = u \cdot v$, then because
$\frac{\partial}{\partial u} u \cdot v = v$ and
$\frac{\partial}{\partial v} u \cdot v = u$, the adjoint rules are
$$
\bar{u} \ \ {\small +}{=} \ \ \bar{w} \cdot v
$$
and
$$
\bar{v} \ \ {\small +}{=} \ \ \bar{w} \cdot u.
$$

## Adjoint propagation and continuations

Adjoint rules increment the adjoints of operands based on the adjoint
of the result, and as such, must be executed after the adjoint of the
result has been computed.  Thus the adjoint rules need to be executed
in the reverse of the order in which they appear.

Computationally, as each operation executes, the adjoint increments
for each operand are pushed onto a stack. After the final value is
computed for which derivatives are required, the adjoints are executed
by popping adjoint rules off the stack and executing them.  The
adjoint code amounts to a continuation, that is a bit of executable
code with references to existing variables.




Computationally, adjoint propagation amounts to executing a
continuation for each operand
such behavior amounts to executing the adjoint rule as a continuation,
with continuations for expressions forming a stack that is executed
last-in-first-out.

the result's
adjoint is known.  This means the adjoint rules have to be applied in
reverse, from the final dependent variable back to the input
independent variables.

For example, consider a simple compound expression $y = \log (u \cdot
v).$  To compute the value of $y$, First, $u \cdot v$ is executed,
then the logarithm of the product is computed.  This can be expressed
with intermediate variables as a function of independent variables $u$
and $v$ as follows, where the numbers in parentheses indicate the
order of steps performed.

$$
\begin{array}{ll||lr}
\textrm{execution}
& \textrm{forward}
& \textrm{reverse}
& \textrm{execution}
\\[-4pt]
\textrm{order} \downarrow
& \textrm{values}
& \textrm{adjoints}
& \textrm{order} \uparrow
\\ \hline
(1) & a = u \cdot v & \bar{u} \ \ {\small +}{=} \ \ \bar{a} \cdot v & (6)
\\
    &               & \bar{v} \ \ {\small +}{=} \ \ \bar{a} \cdot u & (5)
\\ \hline
(2) & b = \log a    & \bar{a} \ \ {\small +}{=} \ \ \bar{b} \cdot \frac{1}{a}
& (4)
\\ \hline
&               & \bar{b} = 1 & (3)
\end{array}
$$

First, the values are computed in a forward pass (steps 1 and 2).
Then the adjoint of the final result is set to one (step 3) and all
other adjoints are initialized to zero.  Then the adjoints are
propagated in a reverse pass (steps 4, 5, and 6).  These steps are
executed with concrete values.  For example, taking $u = 1.2$ and $v =
3.9$, the execution order is as follows, with values rounded to two
decimal places,
$$
\begin{array}{c|rll|c|r}
\textrm{step} & \textrm{variable} & \textrm{op} & \textrm{value} &
\textrm{symbolic} & \textrm{numeric} \\ \hline
& u & = & 1.2 & u & 1.20
\\
& v & = & 3.9 & v & 3.90
\\ \hline
(1) & a & = & u \cdot v & u \cdot v & 4.68
\\
(2) & b & = & \log a & \log v \cdot u & \approx 1.60
\\ \hline
(3) & \bar{b} & = & 1 & 1 & 1.00
\\ \hline
(4) & \bar{a} & {\small +}{=} & \bar{b} \cdot \frac{1}{a}
& \frac{1}{u \cdot v}
& \approx 0.21
\\
(5) & \bar{v} & {\small +}{=} & \bar{a} \cdot u
& \frac{1}{v} & \approx 0.26
\\
(6) & \bar{u} & {\small +}{=} & \bar{a} \cdot v
& \frac{1}{u} & \approx 0.83
\end{array}
$$

Before the algorithm begins, the independent variables $u$ and $v$ are
assumed to be set to their values.  Then steps (1) and (2) calculate
the values of subexpressions, first of $a = u \cdot v$ and then of $b
= \log u \cdot v$, the final result.  To start the reverse pass in
step (3), the final result $b$ has its adjoint $\bar{b}$ set to 1.
The reverse pass then increments the adjoint of each operand involved
in the following expression.  The final adjoints $\bar{u}$ and
$\bar{v}$ are the elements of the gradient of $f(u, v) = \log u \cdot
v$, evaluated at $\begin{bmatrix} 1.2 & 3.9 \end{bmatrix}$,
$$
\nabla f(1.2, 3.9)
=
\begin{bmatrix}
\frac{\partial}{\partial u} \log u \cdot v
&
\frac{\partial}{\partial v} \log u \cdot v
\end{bmatrix}
\approx
\begin{bmatrix}
0.83 & 0.26
\end{bmatrix}.
$$

The results of automatic differentiation can be verified with
analytical derivatives,
$$
\bar{u}
= \frac{\partial}{\partial u} \log u \cdot v
= \frac{1}{u}
\approx 0.83
$$
and
$$
\bar{v}
= \frac{\partial}{\partial v} \log u \cdot v
= \frac{1}{v}
\approx 0.26.
$$


## Matrix derivatives

The adjoint method can be applied to matrix functions in the same way
as to scalar functions.  For example, suppose $C = f(A, B)$, where $A,
B,$ and $C$ are matrices.  The adjoint rules for operands $A$ and $B$
are
$$
\overline{A} \ \ {\small +}{=} \ \
\left( \frac{\partial C}{\partial A} \right)^{\top} \cdot
\overline{C},
$$
and
$$
\overline{B} \ \ {\small +}{=} \ \
\left( \frac{\partial C}{\partial B} \right)^{\top} \cdot
\overline{C}.
$$
The update rule increments the adjoint of the operand by the product
of the derivative of the result with respect to the operand and the
adjoint of the result.

For example, if $C = A + B$, then the adjoint rules are
$$
\overline{A} \ \ {\small +}{=} \ \ \overline{C}
$$
and
$$
\overline{B} \ \ {\small +}{=} \ \ \overline{C}.
$$
because $\frac{\partial}{\partial A}C = 1$ and
$\frac{\partial}{\partial B}C = 1.$

In the more interesting case of multiplication, with $C = A \cdot B$,
the adjoint rules are
$$
\overline{A}  \ \ {\small +}{=} \ \ \overline{C} \cdot B^{\top}
$$
and
$$
\overline{B}  \ \ {\small +}{=} \ \ A^{\top} \cdot \overline{C}.
$$
