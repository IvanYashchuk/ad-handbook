---
knit: 'bookdown::render_book("index.Rmd", "tufte_html_book")'
title: 'Automatic Differentiation Handbook'
author: 'Bob Carpenter, editor'
date: '2019'
bibliography: all.bib
# biblio-style: "acm"
link-citations: true
output:
  bookdown::tufte_html_book:
    split_by: chapter
    toc_depth: 2
  bookdown::tufte_book:
    latex_engine: "pdflatex"
    includes:
      in_header: header.tex
    toc:
      collapse: section
    fontsize: 11pt
---

# Preface {-}

The goal of this book is to introduce the basic algorithms for
automatic differentiation along with an encyclopedic collection of
automatic differentiation rules for popular mathematical and
statistical functions.

## Overview of this book {-}

Automatic differentiation is a general technique for conerting a
function computing values to one that also computes derivatives.
Derivative computations add only a constant overhead to each operation
used to compute the function value, so that the differentiable
function has the same order of complexity as the original function.

After describing the standard forms of automatic differentiation, this
book supplies an encyclopedic collection of tangent and adjoint rules
for forward-mode and backward-mode automatic differentiation, covering
most widely used scalar, vector, matrix, and probability functions.

The appendix contains working example code for forward-mode,
reverse-mode, and mixed-mode automatic differentiation.


## Why derivatives? {-}

Applications of derivatives, which measure the change in one quantity
relative to a change in anoher quantity, are ubiquitous in applied
mathematics.  Although derivatives can be computed mechanistically by
hand in many cases, the process is painstaking and error prone.

The present text is motivated by the fact that most state-of-the-art
statistical inference algorithms are based on first- or higher-order
derivatives.  Examples include

* quasi-Newton optimization for penalized maximum likelihood
estimation (first-order derivatives with approximate second-order
derivatives),
* Laplace approximation for standard errors and approximate Bayesian
posteriors (second-order derivatives),
* Hamiltonian Monte Carlo sampling for Bayesian posterior inference
(first-order derivatives with Euclidean geometry and third-order
derivatives with Riemannian geometry), and
* stochastic gradient descent for nested expectations in variational
inference (Monte Carlo samples of first-order derivatives).

## Why automatic differentiation? {-}

Deriving derivatives analytically by hand is not only painstaking and
error prone under the best of circumstances, it is difficult to do
efficiently for iterative functions involved in matrix factorization
or differential equation solving.  As its name implies, automatic
differentiation automatically lifts a function computing a value to
one computing the value and its derivatives.  Perhaps more
surprisingly, this can be done in full generality with both efficiency
and high arithmetic precision.

## Overview of automatic differentiation techniques {-}

Forward-mode automatic differentiation employs the tangent method of
propagating the chain rule forward from independent (input) variables.
Forward mode computes derivatives of all outputs of a function $f:
\mathbb{R} \rightarrow \mathbb{R}^M$ with respect to its input.
Forward mode can also be used to efficiently compute directional
derivatives, or more generally, gradient-vector products.

Reverse-mode automatic differentiation employs the adjoint method of
propagating the chain rule backward from a dependent (output)
variable.  Reverse-mode computes the gradient of a function, that is
the derivatives of the output a function $f : \mathbb{R}^N \rightarrow
\mathbb{R}$ with respect to each input.

Either forward-mode or reverse-mode automatic differentiation may be
used to compute Jacobians, that is the $M \times N$ matrix of all
first-order derivatives of a function $f:\mathbb{R}^N \rightarrow
\mathbb{R}^M.$ Jacobians require $N$ passes of forward-mode or $M$
passes of reverse-mode automatic differentiation.

Nesting reverse-mode automatic differentation within forward mode
provides efficient calculation of Hessians, that is the matrix of all
second-order derivatives of a function $f:\mathbb{R}^N \rightarrow
\mathbb{R}.$ Hessians require $N$ passes of reverse mode nested in
forward mode.  Just as forward-mode automatic differentiation may be
used to efficiently calculate gradient-vector products, reverse mode
nested in forward mode can be used to compute Hessian-vector products
efficiently.
